from typing import Dict, List
from uuid import UUID
import streamlit as st
import time

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter , RecursiveCharacterTextSplitter

from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.vectorstores import Chroma, FAISS
from langchain.storage import LocalFileStore
from langchain.callbacks.base import BaseCallbackHandler
from langchain.prompts import MessagesPlaceholder
from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory

from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings, OllamaEmbeddings
from langchain.chat_models import ChatOpenAI, ChatOllama



# ì´ˆê¸°í™” ---------------------------------------------------------------  

page_title = "PrivateGPT"
st.set_page_config(
    page_title=page_title,
    page_icon="ğŸ”’",
)

# ì„¸ì…˜ì´ˆê¸°í™”     
if "messages" not in st.session_state : 
    st.session_state["messages"] = []
if "memory" not in st.session_state :     
    st.session_state["memory"] = None
    
    
# í•¨ìˆ˜ì •ì˜ ---------------------------------------------------------------       

#@st.cahe_data : fileì´ ì´ì „fileëª…ê³¼ ì¼ì¹˜í•  ê²½ìš°, í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šê³  ì´ì „ ê²°ê³¼ê°’ì„ ë°˜í™˜í•œë‹¤.
@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    '''
    ì‚¬ìš©ì ì…ë ¥íŒŒì¼ì„ ìºì‰¬í´ë”ì— ì €ì¥í›„, ì„ë² ë”© ë° retriever ìˆ˜í–‰
    '''
    
    # ì…ë ¥ë°›ì€ íŒŒì¼ë‚´ìš© ì½ê¸°
    file_content = file.read()
    
    # ì…ë ¥ë°›ì€ íŒŒì¼ì„ ìºì‰¬í´ë”ì— ì €ì¥
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f :
        f.write(file_content)
    
    # ìºì‰¬í´ë”ì— ì €ì¥ëœ íŒŒì¼ ë¡œë“œ 
    loader= UnstructuredFileLoader(file_path)
    
    # ìŠ¤í”Œë¦¿í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n", 
        chunk_size = 600,
        chunk_overlap = 100,
    )
    docs = loader.load_and_split(text_splitter = splitter)
    
    # ì„ë² ë”©í•¨ìˆ˜ í˜¸ì¶œ
    embeddings = OpenAIEmbeddings()
    
    # ì„ë°°ë”©ê²°ê³¼ë¥¼ ì €ì¥í•  ìºì‰¬í´ë” ìƒì„±
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    
    # ìºì‰¬ì„ë°°ë”©í•¨ìˆ˜ ìƒì„±
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ì‚¬ìš© >> ì„ë°°ë”©í›„ >> íŒŒì¼ê²½ë¡œ ê²€ìƒ‰ >> ìºì‰¬í´ë” ê²€ì‚¬ >> ê²€ìƒ‰ìˆ˜í–‰ í›„ ìºì‰¬í´ë”ì— ì €ì¥ or ìºì‰¬ë°ì´í„° ì‚¬ìš©
    vectorstore = FAISS.from_documents(docs,cached_embeddings)
    retriever = vectorstore.as_retriever()
    
    return retriever

def save_message(message, role):
    st.session_state["messages"].append({"message":message, "role": role})
    
def send_message(message, role, save=True):
    '''
    ë©”ì„¸ì§€ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ê³  ì„¸ì…˜ì— ì €ì¥
    '''
    # ë©”ì„¸ì§€ë¥¼ í™”ë©´ì— ì¶œë ¥
    with st.chat_message(role):
        st.markdown(message)
        
    # ì„¸ì…˜ì— ë©”ì„¸ì§€ ëˆ„ì 
    if save :
        save_message(message, role)
        
def paint_history():        
    '''
    íˆìŠ¤í† ë¦¬ í™”ë©´ ìƒì„±
    '''
    # ì„¸ì…˜ì— ì €ì¥ëœ ë‚´ì—­ì¶œë ¥ ë° ì„¸ì…˜ ì¤‘ë³µì €ì¥ False
    for message in st.session_state["messages"] : 
        send_message(message["message"],message['role'], save=False)
    
def format_docs(docs):
    '''
    ë¦¬íŠ¸ë¦¬ë²„ì— ì˜í•´ ê²€ìƒ‰ëœ ë¬¸ì„œì§‘í•©ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ í†µí•©
    '''
    return "\n\n".join(document.page_content for document in docs)


    
    
class ChatCallbackHandler(BaseCallbackHandler):
    '''
    callback handler : 
    LangChainì˜ context ì•ˆì— ìˆëŠ” í´ë˜ìŠ¤ë¡œì„œ,llmì˜ eventë¥¼ listen í•¨.
    ë‹µë³€ì„ ì¶œë ¥í• ë•Œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ì£¼ê¸°ìœ„í•´ ìƒì„±
    '''    
    message_tk = ""
   
    def on_llm_start(self, *args, **kwargs) : 
         self.message_box = st.empty()
            
    def on_llm_end(self,*args, **kwargs):
        with st.sidebar:
            st.write("llm ended!")
            
        save_message(self.message_tk, "ai")
        
            
    def on_llm_new_token(self, token, *args, **kwargs):
        
        print(token)
        self.message_tk += token
        
        # ì „ì²´ ë©”ì„¸ì§€ì—ì„œ ìš”ì•½ë¶€ë¶„ ì œê±° 
        self.message_box.markdown(self.message_tk)


# ìœ„ ì²´ì¸ ëŒ€ì‹ , LCELì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤. 

def load_memory(_):
    return memory.load_memory_variables({})["history"]

def invoke_chain(question) :
    
    result = chain.invoke(question)
    
    memory.save_context(
        {"input": question},
        {"output": result.content}
        )# ì´ ë¶€ë¶„ì„ db ì— ì €ì¥í•  ìˆ˜ë„ ìˆë‹¤.
    st.session_state["memory"] = memory
    
    return result
        
# ê¸°ë³¸ë ˆì´ì–´ ë¶€ --------------------------------------------------------  

st.markdown("""
            # ğŸ”’PrivateGPT
            ë¡œì»¬í™˜ê²½ì—ì„œ ì‘ë™í•˜ëŠ” AIì±—ë´‡ì…ë‹ˆë‹¤.    
            ì‚¬ì´ë“œë°”ì— íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì„¸ìš”.            
            """)

with st.sidebar : 

    file_load_flag = True
    api_key = st.session_state.get("api_key", None)

    if api_key: 
        if st.session_state.api_key_check :
            st.success("âœ”ï¸ API confirmed successfully.")  
            file_load_flag = False
            
        else : 
            st.warning("Please enter your API key on the main(home) page.")
    else:
        st.warning("Please enter your API key on the main(home) page.") 
        
    file  = st.file_uploader("Upload a .txt .pdf or .docx file", type = ["pdf", "txt", "docx"], disabled=file_load_flag)
    
    st.components.v1.html(
    """
    <div style = "margin : 50px 0 0 -5px">
     <script type="text/javascript" 
    src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" 
    data-name="bmc-button" 
    data-slug="cj5311" 
    data-color="#FFDD00" 
    data-emoji="" 
    data-font="Cookie" 
    data-text="Buy me a coffee" 
    data-outline-color="#000000" 
    data-font-color="#000000" 
    data-coffee-color="#ffffff"
   >
    </script>
    </div>

    """,
    )

# ì‚¬ìš©ì ì…ë ¥ íŒŒì¼ì´ ìˆì„ë•Œ
if file : 
    llm = ChatOllama(
    model = "mistral:latest",
    # model ="gemma2:2b ",
    temperature = 0.1,
    streaming = True, # ë¬¸ì íƒ€ì´í•‘ í”Œë ˆì´ íš¨ê³¼, ì¼ë¶€ëª¨ë¸ì—ì„œëŠ” ì§€ì›ì•ˆí•¨
    # callbacks = [ChatCallbackHandler()]
    ) 


    # mistral ì´ instructor ê¸°ë°˜ì´ê¸° ë•Œë¬¸ì— ìŠ¤íŠ¸ë§ìœ¼ë¡œ ë³€í™˜
    prompt = ChatPromptTemplate.from_messages({
        """
        Answer the question using ONLY the following context and not your training data.
        If you don't know the answer just say you don't know.
        Don't make anything up.
        
        Context: {context} 
        Question: {question}
        """
    }
    )


    if st.session_state["memory"] is None:
        
        st.session_state["memory"] = ConversationBufferMemory(
                                        llm = llm,
                                        max_token_limit = 150,
                                        return_messages=True
                                    )


    memory = st.session_state["memory"]
    # rag ìˆ˜í–‰
    retriever =  embed_file(file)

    # ai ì•ˆë‚´ë¬¸ ì¶œë ¥ 
    send_message("I'm ready! Ask away!", "ai", save=False)
    
    # ê³¼ê±° ë©”ì„¸ì§€ë“¤ í™”ë©´ ì¶œë ¥ 
    paint_history()
    
    # ì‚¬ìš©ì ì§ˆë¬¸ì…ë ¥ì°½ ë°œìƒ
    message = st.chat_input("Ask anything about your file...")
    
    # ì‚¬ìš©ì ì§ˆë¬¸ ë“¤ì–´ì˜¬ë•Œ
    if message : 
        
        # ì‚¬ìš©ì ì§ˆë¬¸ì„ í™”ë©´ì— ì¶œë ¥
        send_message(message, "human")
        
        chain =  {
            "context" : retriever | RunnableLambda(format_docs),
            "question" : RunnablePassthrough(),
            "history" : load_memory
            }  | prompt | llm
        
        with st.chat_message("ai"):
            response = invoke_chain(message)
            
        # st.session_state
        
    else : 
        # íŒŒì¼ì„ ì¬ì—…ë°ì´íŠ¸ í–ˆì„ë•Œ ì„¸ì…˜ ì´ˆê¸°í™”
        st.session_state["messages"] = []
        st.session_state["memory"] = None
        
        

    
        
